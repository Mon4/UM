{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Uczenie maszynowe - lab 6 - Enkodery\n",
    "\n",
    "Monika Etrych"
   ],
   "id": "6d391b8cd1a8dfe0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. AutoEnkoder (AE)",
   "id": "f0b949834e5d8c27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zadanie 1.1 - Dlaczego sigmoid jest odpowiednią funkcją aktywacji w ostatniej warstwie dekodera w tym przypadku? (0.25pkt)\n",
    "\n",
    "Funkcja sigmoid zwraca wartość z przedziału [0, 1].  W naszym przypadku używamy sigmoid aby wyjściowe obrazy były zakodowane tak jak wejściowe - tzn. wartości ciągłe.\n"
   ],
   "id": "7ac3664f3edc94da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Zaimplementuj funkcję do wizualizacji reprezentacji obrazów ze zbioru testowego w ukrytej przestrzeni 2-wymiarowej. Wyświetl wizualizację.",
   "id": "f36b1c9092d6a65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_latent_space(model, data):\n",
    "    encoded = model.encoder(data)\n",
    "    \n",
    "    data_reshaped = encoded.reshape(data.shape[0], -1)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    latent_2d = pca.fit_transform(data_reshaped)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c='green', alpha=0.8)\n",
    "    plt.title('Latent Space Plot - AE')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "bd216bf75c125462"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![img1](./img/1.PNG)",
   "id": "53c90c7213ece154"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zadanie 1.3. Wybierz ze zbioru testowego dwa obrazy z różnymi liczbami. Dobierz takie liczby, dla których spodziewasz się, że odkodowanie średniej z ich zenkodowanych reprezentacji będzie miało sens. Wybierz dwie takie pary.  \n",
    "Dla każdej z par:  \n",
    "* Wyświetl wybrane liczby.  \n",
    "* Użyj enkodera do uzyskania 2-wymiarowych reprezentacji każdej liczby.  \n",
    "* Wylicz średnią z tych reprezentacji.  \n",
    "* Użyj dekodera na uzyskanej średniej.  \n",
    "* Wyświetl wynik.\n",
    "* Skomentuj wynik - czy przypomina jakąś liczbę? Czy takiego wyniku się spodziewałaś/eś?  \n",
    "\n",
    "(0.25pkt)\n",
    "\n",
    "\n",
    "Łączna liczba punktów do uzyskania za tę część: 0.75"
   ],
   "id": "59421cdc484f4fcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "images = [x_test[0], x_test[2], x_test[4], x_test[9]]\n",
    "\n",
    "n = len(images)\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images[i], cmap='Greys_r')\n",
    "    if i < 2: plt.title(\"pair 1\") \n",
    "    else: plt.title(\"pair 2\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ],
   "id": "d4d892e09d3caab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![img2](./img/2.PNG)",
   "id": "e5fb663abb510e1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "seven = autoencoder.encoder.predict(np.expand_dims(images[0], axis=0))\n",
    "two = autoencoder.encoder.predict(np.expand_dims(images[1], axis=0))\n",
    "four = autoencoder.encoder.predict(np.expand_dims(images[2], axis=0))\n",
    "nine = autoencoder.encoder.predict(np.expand_dims(images[3], axis=0))\n",
    "\n",
    "seven_two_mean = np.mean(np.array([seven, two]), axis=0 )\n",
    "four_nine_mean = np.mean(np.array([four, nine]), axis=0 )\n",
    "\n",
    "seven_two_dec = autoencoder.decoder.predict(seven_two_mean)\n",
    "four_nine_dec = autoencoder.decoder.predict(four_nine_mean)"
   ],
   "id": "c33c3bf11c923246"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "images_dec = [np.squeeze(seven_two_dec), np.squeeze(four_nine_dec)]\n",
    "\n",
    "n = len(images_dec)\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images_dec[i], cmap='Greys_r')\n",
    "    if i < 1: plt.title(\"pair 1\") \n",
    "    else: plt.title(\"pair 2\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ],
   "id": "5caf9dbcb59b41a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![img3](./img/3.PNG)",
   "id": "8c876f1e53ebe9f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. AutoEnkoder wariacyjny (VAE) ",
   "id": "637827a40931535d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zadanie 2.1. Dlaczego powyższa implementacje CVAE nie stosuje żadnej aktywacji w ostatniej warstwie enkodera? Czy jakaś funkcja by się tutaj nadawała? (0.25pkt)\n",
    "\n",
    "Ponieważ model uczy się generować nowe obrazy na podstawie rozkładu prawdopodobieństwa obliczanego na podstawie binarnej reprezentacji, to wyjściowe dane niekoniecznie muszą być z zakresu [0, 1]. Nie ustawianie funkcji aktywacji w wartstwie wyjściowej daje możliwość uzyskania bardziej zróżnicowanych wyników przez co model ma większe możliwości generatywne. Później i tak dokonuje się zaokrąglenia aby uzyskać ponownie reprezemtację binarną (jeśli to jest naszym celem).\n"
   ],
   "id": "1a36acfe004a0f5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Narysujmy teraz, podobnie jak wcześniej dla AE, dwuwymiarową przestrzeń ukrytą. Zaimplementuj funkcję plot_latent_space, która zenkoduje zbiór danych, a następnie wyświetli każdy punkt wraz z odchyleniem standardowym.",
   "id": "29283163c37d2486"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_latent_space(model, data):\n",
    "    mean, log_var = model.encode(data)\n",
    "    \n",
    "    std_dev = tf.exp(0.5 * log_var)\n",
    "    \n",
    "    epsilon = tf.random.normal(tf.shape(std_dev))\n",
    "    \n",
    "    # normalization\n",
    "    z = mean + epsilon * std_dev \n",
    "    z = z.numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(z[:, 0], z[:, 1], c='blue', alpha=0.8)\n",
    "    plt.title('Latent Space Plot - CVAE')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "f4e14aa0eeab8fb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![img4](./img/4.png)",
   "id": "8567370e0a303833"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Zadanie 2.2. Skomentuj wynik uzyskany przy użyciu funkcji plot_latent_images. Zwróć uwagę na jakość/sensowność rysowanych liczb. Porównaj wykres do analogicznego wykresu dla modelu AE. Zamieść w raporcie wykresy. (0.25pkt)  \n",
    "### Autoenkoder\n",
    "![5](./img/5.png)\n",
    "### CVAE\n",
    "![6](./img/6.png)\n",
    "\n",
    "Wyniki uzyskane przez CVAE są mniej czytelne, trudniej domyśleć się jaką cyfrę wygenerował model.\n",
    "\n",
    "\n",
    "  \n",
    "#### Zadanie 2.3. Porównaj wyniki funkcji _plot_latent_space_ dla AE oraz VAE. Zwróć uwagę na \"gęstość\" punktów oraz zakres wartości. Zamieść w raporcie wykresy. (0.25pkt)   \n",
    "  \n",
    "W AE zakres wartości jest szerszy. W VAE punkty są gęściej rozmieszczone.\n",
    "\n",
    "#### Zadanie 2.4. Dla tych samych par obrazów, na których pracowałaś/eś w ostatnim zadaniu dot. AE, przygotuj reprezentacje ukryte z pomocą wytrenowanego VAE i odkoduj średnie z reprezentacji. Skomentuj wyniki, porównaj z wynikami z AE. (0.25pkt)  \n",
    "\n",
    "![7](./img/7.png)\n",
    "\n",
    "Wyniki wyszły bardzo podobne do AE, a to dlatego, że enkoder w obu przypadkach jest bardzo podobny do siebie, różnią się jedynie ostatnią warstwą. AE zwraca wektor o długości latent_dim, a VAE dwuktotny taki wektor.\n",
    "\n",
    "#### Zadanie 2.5. Wróć do funkcji _compute_loss_. Człony _logpz_ oraz _logqz\\_x_ związane są z obliczaniem KL-divergence pomiędzy $Q(z|X)$ oraz $P(z)$. Zakładamy, że oba te rozkłady są gaussowskie, stąd możemy wykorzystać wzór na KL-divergence dla dwóch rozkładów gaussowskich. Znajdź ten wzór oraz przepisz funkcję _compute_loss_ z jego wykorzystaniem. Zamieść w raporcie przygotowaną formułę. Wytrenuj model ponownie, porównaj wyniki z poprzednią implementacją _compute_loss_. (0.25pkt)\n",
    "\n",
    "Wychodząc od wzoru na KL dla rozkładów ciągłych:\n",
    "\n",
    "![formula](./img/f1.png)\n",
    "\n",
    "Podstawiajać za rozkłady funkcje gęstości:\n",
    "\n",
    "![formula](./img/f2.png)\n",
    "\n",
    "Otrzymujemy:\n",
    "\n",
    "![formula](./img/8.png)\n",
    "\n",
    "Przy dodatkowym założeniu, że rozkład P(X) jest normalny zestandaryzowany mean1=0, var1=1, uproszczony wzór to:\n",
    "\n",
    "![formula](./img/f3.png)\n",
    "\n",
    "Dodatkowo wzór na loss:\n",
    "\n",
    "![formula](./img/9.png)\n"
   ],
   "id": "1e48ad4bb878c968"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_loss(model, x):\n",
    "  mean, logvar = model.encode(x)\n",
    "  z = model.reparameterize(mean, logvar) # draw examples\n",
    "  x_logit = model.decode(z)\n",
    "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x) \n",
    "  \n",
    "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3]) # error\n",
    "  kl_div = -0.5 * tf.reduce_sum(1 + logvar - tf.exp(logvar) - tf.square(mean), axis=-1)\n",
    "  \n",
    "  return -tf.reduce_mean(logpx_z - kl_div)"
   ],
   "id": "1d35b6d3a3a9be05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Wyniki\n",
    "\n",
    "Oryginala implementacja:\n",
    "\n",
    "![formula](./img/r1.png)\n",
    "\n",
    "Zmieniona implementacja:\n",
    "\n",
    "![formula](./img/r2.png)\n",
    "\n",
    "Wyniki wychodzą takie same."
   ],
   "id": "98cfac05b1e48ae0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Conditioned CVAE\n",
    "Uzupełnij klasę Cond_CVAE na podstawie klasy CVAE. W tym celu:  \n",
    "\n",
    "Uzupełnij funkcję, która z każdego obrazu ze zbioru danych x tworzy 9 obrazów z cyfrą na każdej z 9 pozycji siatki 3x3, a także tworzy etykiety y w postaci wektora [cyfra-one-hot, pozycja_x, pozycja_y]. Dla każdej pary z oryginalnego zbioru danych (obraz, etykieta) wylosuj num_imgs par, które znajdą się w docelowym zbiorze danych (nie zapisujemy wszystkich 9 możliwości ze względu na ograniczenia RAM)."
   ],
   "id": "217449d182eac2b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "def conditioned_mnist(x, y, num_imgs=2):\n",
    "  x_res = np.empty(shape=(num_imgs*len(x), 42, 42), dtype='float32')  # pusta macierz z wynikami - obrazy x\n",
    "  y_res = np.empty(shape=(num_imgs*len(y), 12), dtype='float32')  # pusta macierz z wynikami - wektor y: etykieta (10 liczb), pozycja x, pozycja y\n",
    "  empty_res = np.zeros(shape=(42, 42))  # obraz wynikowy w docelowym rozmiarze, wypełniony zerami\n",
    "\n",
    "  for el, (arr, label) in enumerate(zip(x, y)):\n",
    "    to_sample_x = np.empty((9, x.shape[1]*3, x.shape[2]*3), dtype='float32')  # macierz przechowująca 9 wersji obrazu\n",
    "    to_sample_y = np.empty((9, 12), dtype='float32')  # macierz przechowująca 9 wersji etykiet\n",
    "    for i in range(3):\n",
    "      for j in range(3):\n",
    "        curr_x = empty_res.copy()\n",
    "        curr_x[i*x.shape[1]: (i+1)*x.shape[1], j*x.shape[2]: (j+1)*x.shape[2]] = arr.reshape((x.shape[1], x.shape[2]))\n",
    "        curr_y = [*label, i/2, j/2]  # normalizacja\n",
    "        to_sample_x[3*i+j] = curr_x\n",
    "        to_sample_y[3*i+j] = curr_y\n",
    "    idxs = [random.randint(0, 8) for i in range(num_imgs)]  # wylosuj num_imgs indeksów z zakresu [0; 8] jako wektor numpy\n",
    "    x_res[el*num_imgs: (el+1)*num_imgs] = [to_sample_x[i] for i in idxs]\n",
    "    y_res[el*num_imgs: (el+1)*num_imgs] = [to_sample_y[i] for i in idxs]\n",
    "  x_res = x_res.reshape((-1, x.shape[1]*3, x.shape[2]*3, 1))\n",
    "  return x_res, y_res"
   ],
   "id": "a82f22c121c9ac8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Uzupełnij funkcję prepare_encoder. Będziemy mieć dwa wejścia do modelu - jedno na obraz, jedno na wektor cech [etykieta, pos_x, pos_y]. Przeprocesuj obraz z pomocą warstw konwolucyjnych (możesz wykorzystać implementację z CVAE). Możesz też przygotować kilka warstw, które zajmą się wektorem cech. Użyj warstwy konkatenującej wyniki z przetwarzania obrazu i wektora cech. Za tą warstwą znajdzie się warstwa gęsta, wyliczająca średnią i logvar.\n",
    "\n",
    "2. Uzupełnij funkcję prepare_decoder. Tu również mamy do czynienia z dwoma wejściami - jedno przyjmuje szum, drugie wektor cech. Połącz oba wejścia i przygotuj dekoder. Możesz skorzystać z implementacji CVAE, ale będą potrzebne zmiany związane z innym rozmiarem obrazów.\n",
    "\n",
    "Pozostałe funkcje są już zaimplementowane. Przyjrzyj się im. Co się zmieniło względem implementacji CVAE?"
   ],
   "id": "30a2547d5e838e32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def prepare_encoder(self):\n",
    "    input_img = tf.keras.layers.Input(shape=(42, 42, 1))\n",
    "    input_cond = tf.keras.layers.Input(shape=(12, ))\n",
    "    conv_img = tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(3, 3), activation='relu')(input_img)\n",
    "    conv_img = tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(3, 3), activation='relu')(conv_img)\n",
    "    flatten_img = tf.keras.layers.Flatten()(conv_img)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([flatten_img, input_cond])\n",
    "    # No activation\n",
    "    x = tf.keras.layers.Dense(latent_dim + latent_dim)(x)\n",
    "    return  tf.keras.Model([input_img, input_cond], [x])\n",
    "\n",
    "def prepare_decoder(self):\n",
    "    input_latent = tf.keras.layers.Input(shape=(latent_dim,))  # noice \n",
    "    input_cond = tf.keras.layers.Input(shape=(12, ))  # labels, pos\n",
    "    inputs = tf.keras.layers.Concatenate()([input_latent, input_cond])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units=7*7*64, activation=tf.nn.relu)(inputs)\n",
    "    x = tf.keras.layers.Reshape(target_shape=(7, 7, 64))(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(\n",
    "            filters=64, kernel_size=3, strides=2, padding='same',\n",
    "            activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(\n",
    "            filters=32, kernel_size=3, strides=2, padding='same',\n",
    "            activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32, kernel_size=3, strides=2, padding='same',\n",
    "        activation='relu')(x)\n",
    "    \n",
    "    return  tf.keras.Model([input_latent, input_cond], [x])"
   ],
   "id": "598c5dbe89235460"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "W pozostałych funkcjach (a konkretniej w funkcji sample) uwzględniono wektor cech - cond.",
   "id": "10cc046002722b16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Funkcja kosztu",
   "id": "d74b03d41d381077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_loss(model, x):\n",
    "  data, cond = x\n",
    "  mean, logvar = model.encode(x)\n",
    "  z = model.reparameterize(mean, logvar)\n",
    "  x_logit = model.decode(z, cond)\n",
    "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=data)\n",
    "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "  logpz = log_normal_pdf(z, 0., 0.)\n",
    "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)"
   ],
   "id": "cf360cafad941090"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zadanie 3.1. Sprawdź jakość modelu dla 3 różnych wartości latent_dim (trzeba dla każdej z nich osobno wytrenować model). Niech będą od siebie znacząco różne, np. 2, 25, 100. Przy większym latent_dim może być potrzebnych więcej epok.  \n",
    "Punktacja:  \n",
    "Skuteczny trening dla jednej wartości latent dim = 1 pkt.  \n",
    "0.5 pkt za testy dla dwóch innych wartości latent_dim (po 0.25 za każdą). Punkty będą przyznane, jeśli model będzie w stanie generować _sensowne_ wyniki.  \n",
    "  \n",
    "Zadanie 3.2. Wykonaj dla najlepszego modelu z punktu 3.1.:\n",
    "* Wybierz przykład ze zbioru testowego (obraz + etykieta).  \n",
    "* Przepuść próbkę przez enkoder, uzyskaj reprezentację _z_.  \n",
    "* Dla każdego z 9 możliwych wektorów [poprawna_etykieta, pos_x, pos_y] przepuść przez dekoder reprezentację _z_ wraz z informacją o etykiecie i położeniu. Wyświetl uzyskany obraz. Skomentuj wyniki - czy za każdym razem uzyskano oczekiwaną liczbę w oczekiwanym miejscu? Jeśli nie, to co może być przyczyną? (0.25 pkt)  \n",
    "\n",
    "Zadanie 3.3. Powtórz zadanie 3.2, ale tym razem jako reprezentację _z_ wykorzystaj wartości wylosowane z rozkładu normalnego oraz wybierz dowolną etykietę. Skomentuj wyniki - czy za każdym razem uzyskano oczekiwaną liczbę w oczekiwanym miejscu? (0.25 pkt)\n",
    "\n",
    "\n",
    "Za tę część: 2 punkty"
   ],
   "id": "d4b0b1e635b84cc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7882cf7c01f4967e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conditioned GAN\n",
    "Zadanie 4.1. \n",
    "Wygeneruj po jednym obrazie z każdą liczbą z pomocą generatora. Oceń jakość wyników. Jeśli jakość modelu pozostawia wiele do życzenia, spróbuj go poprawić, np. zwiększając liczbę epok bądź zmieniając definicję generatora/dyskryminatora.  \n",
    "Punktacja: 1 pkt za poprawnie przeprowadzony trening i wykonane zadanie 4.1."
   ],
   "id": "9cc2880dacb7e7c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d1efe3e244787042"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
